# Instruction Fine-tuning
Instruction fine-tuning Mistral 7B on dolly-15K dataset

# Techniques
- LoRA (Low-Rank Adaptation): rank = 8, alpha = 16, dropout = 0.1
- Quantization: 4-bit quantization
  
# Hardware
- A single GPU P100, equipped with 16GB of memory 
